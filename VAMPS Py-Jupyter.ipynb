{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Panel-01\n",
    "PLAN to upload a fasta file (single or multiple dataset)\n",
    "DEMULTIPLEX if multi-dataset\n",
    "Place resulting fasta files in sub-directory \n",
    "UNIQUE each subsequent file\n",
    "Assign taxonomy to each: go with the easiest first: RDP\n",
    "Then put the sequences, taxonomy, project and dataset into the database\n",
    "\"\"\"\n",
    "import os,sys\n",
    "import shutil\n",
    "#https://pypi.python.org/pypi/pyfasta/\n",
    "from pyfasta import Fasta\n",
    "from bin.remote_db_connector import MySQL_Connect\n",
    "from bin.parse_rdp import RDP_File_Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Panel-02\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# INPUT FASTA FILE INFO:\n",
    "# Input fasta file (relative path)\n",
    "# options are 'single' or 'multi' -dataset\n",
    "file_info = {\"name\":'./test25.fa',\"type\":\"single\",\"sep\":\"|\"}\n",
    "#file_info = {\"name\":'./multi_ds.fa',\"type\":\"multi\",\"sep\":\" \"}\n",
    "\n",
    "# PROJECT INFORMATION\n",
    "prj_info = {\"name\":\"t18\",  # REQUIRED \n",
    "            \"public\":1,       # 1=public; 0=private --Defaults to 1\n",
    "            \"owner\":\"admin\",   # REQUIRED Owner must have vamps_user_name \n",
    "            \"description\":\"testing desc\",\n",
    "            \"title\":\"testing title\",\n",
    "            \"funding\":\"01234\"\n",
    "           }\n",
    "dataset_name = 'ds1'             # ONLY USED IF file_info_type = 'single'\n",
    "taxonomy_assigner = 'rdp'        # also to include 'gast', 'rdp', 'spingo'\n",
    "verbose = False                  # True or False: controls ammount of info output\n",
    "base_dir = os.getcwd()\n",
    "work_dir = os.path.join(base_dir,'work') # sub-directory where files will be (create this by hand!)\n",
    "path_to_rdp_classifier = ''\n",
    "path_to_gast = ''\n",
    "path_to_spingo = ''\n",
    "\n",
    "# MySQL DB CONNECTION INFORMATION\n",
    "db_info = {\"host\":\"localhost\",\"name\":\"vamps_development\",\"user\":\"ruby\",\"pword\":\"ruby\"}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Database Connection:\n",
      "(44, 'ICM_SPO_Ev9', 'Title', 'Project Descriptionx', '6vB_OPS_MCI', 'myfunding', 48, 1)\n",
      "(46, 'ICM_LCY_Bv6', 'Title', 'Project Descriptions', '6vB_YCL_MCI', 'myfunding', 48, 1)\n",
      "Done; Connection Success\n",
      "Project already exists: t18\n",
      "Changing project name to: t18_637\n",
      "Validated VAMPS username:\t admin (userID:48)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Panel-03\n",
    "VALIDATE DATABASE CONNECTION\n",
    "CHECK PROJECT NAME\n",
    "CHECK VAMPS USERNAME\n",
    "\"\"\"\n",
    "conn_success = True\n",
    "print('Testing Database Connection:')\n",
    "try:\n",
    "    db = MySQL_Connect(HOST=db_info[\"host\"], DB=db_info[\"name\"], USER=db_info[\"user\"], PWORD=db_info[\"pword\"])\n",
    "    q = \"SELECT * FROM project limit 2\"\n",
    "    db.cur.execute(q)\n",
    "    result = db.cur.fetchall()\n",
    "    for item in result:\n",
    "        print(item)\n",
    "    print('Done; Connection Success')\n",
    "except:\n",
    "    sys.exit('CONNECTION FAILURE --Exiting')\n",
    "    conn_success = False\n",
    "\n",
    "if conn_success:\n",
    "    \"\"\" PROJECT \"\"\"\n",
    "    q = \"SELECT project_id FROM project WHERE project='\"+prj_info['name']+\"'\"    \n",
    "    db.cur.execute(q)\n",
    "    numrows = int(db.cur.rowcount)\n",
    "    if numrows > 0:\n",
    "        from random import randint\n",
    "        print('Project already exists: '+prj_info['name'])\n",
    "        prj_info['name'] = prj_info['name']+'_'+str(randint(100,999))\n",
    "        print('Changing project name to:',prj_info['name'])\n",
    "    else:\n",
    "        print('Validated project name:\\t\\t',prj_info['name'])\n",
    "    \n",
    "    \"\"\" OWNER \"\"\"\n",
    "    q = \"SELECT user_id FROM user WHERE username='\"+prj_info['owner']+\"'\"\n",
    "    db.cur.execute(q)\n",
    "    numrows = int(db.cur.rowcount)\n",
    "    if numrows==0:\n",
    "        sys.exit('COULD NOT FIND VAMPS USER:',prj_info['owner']+' --Exiting')\n",
    "    else:\n",
    "        row = db.cur.fetchone()\n",
    "        prj_info['owner_id'] = row[0]\n",
    "        print('Validated VAMPS username:\\t',prj_info['owner'],'(userID:'+str(prj_info['owner_id'])+')')\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running....\n",
      "First 5 complete deflines\n",
      "Your selection (see Panel-02 above)--> file_type: 'single'; separator: '|'\n",
      "Examine these for file type and separator (Change above if needed):\n",
      "  FX18VMI01C550A|rank=0000078|x=1183.0|y=1112.0|length=95\n",
      "  FX18VMI01COJBD|rank=0000195|x=982.0|y=1847.0|length=104\n",
      "  FX18VMI01BZB8B|rank=0000074|x=695.0|y=1817.0|length=104\n",
      "  FX18VMI01C5C71|rank=0000100|x=1174.0|y=671.0|length=106\n",
      "  FX18VMI01A0USI|rank=0000201|x=302.0|y=2640.0|length=104\n",
      "avg_seq_length: 100.93333333333334 bp\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Panel-04\n",
    "CONFIRM/IDENTIFY FASTA TYPE: single or multi\n",
    "Examine deflines: is dataset name first (multi) or sequenceID first (single)? \n",
    "Alter variable to suit: file_info.type (see Panel-02 above)\n",
    "Alter variable to suit: file_info.sep (see above)\n",
    "Also get total_seq_count here\n",
    "\"\"\"\n",
    "print('Running....')\n",
    "print_count = 5  # show only this many\n",
    "print('First',str(print_count),'complete deflines')\n",
    "print(\"Your selection (see Panel-02 above)--> file_type: '\"+file_info[\"type\"]+ \"'; separator: '\"+file_info[\"sep\"]+\"'\")\n",
    "print('Examine these for file type and separator (Change above if needed):')\n",
    "total_seq_count = 0\n",
    "f = Fasta(file_info[\"name\"])\n",
    "avg_seq_length = 0\n",
    "for defline in f:\n",
    "    if total_seq_count < print_count:\n",
    "        print(' ',defline)\n",
    "    avg_seq_length += len(f[defline]) \n",
    "    total_seq_count += 1\n",
    "avg_seq_length = avg_seq_length/total_seq_count\n",
    "print('avg_seq_length:',avg_seq_length,'bp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running....\n",
      "Printing a single seqID and dataset name:\n",
      "Do they look properly formated? -- if not go back and adjust the splits above.\n",
      "representative seqid:\t FX18VMI01C550A\n",
      "representative dataset:\t ds1\n",
      "There are 1 datasets and 30 total sequences\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Panel-05\n",
    "DEMULTIPLEX (If multi dataset) Files.\n",
    "\"\"\"\n",
    "print('Running....')\n",
    "dataset_lookup = {}     # dataset_lookup[ds] = ds_count\n",
    "sequence_collector = {} # sequences[ds][seqid] = seq\n",
    "n=0\n",
    "for defline in f:\n",
    "    # create new fasta here for mothur to unique, names\n",
    "    #print(defline)\n",
    "    items = defline.split(file_info[\"sep\"])\n",
    "    \n",
    "    seq = f[defline]\n",
    "    #print(seq)\n",
    "  \n",
    "    if file_info[\"type\"] == 'multi':\n",
    "        \n",
    "        seqid = items[1].replace(':','_')\n",
    "        ds = items[0].split('_')[0]\n",
    "        if ds in dataset_lookup:\n",
    "            dataset_lookup[ds] += 1\n",
    "        else:\n",
    "            dataset_lookup[ds] = 1\n",
    "        if ds in sequence_collector:\n",
    "            sequence_collector[ds][seqid] = seq\n",
    "        else:\n",
    "            sequence_collector[ds] = {}\n",
    "            sequence_collector[ds][seqid] = seq\n",
    "        \n",
    "    else:  # single dataset format\n",
    "        #print(\"single ds\")\n",
    "        dataset_lookup[dataset_name] = total_seq_count\n",
    "        seqid = items[0]\n",
    "        ds = dataset_name\n",
    "        if dataset_name in sequence_collector:\n",
    "            sequence_collector[ds][seqid] = seq\n",
    "        else:\n",
    "            sequence_collector[ds] = {}\n",
    "            sequence_collector[ds][seqid] = seq\n",
    "    if n == 0:\n",
    "        print('Printing a single seqID and dataset name:')\n",
    "        print('Do they look properly formated? -- if not go back and adjust the splits above.')\n",
    "        print('representative seqid:\\t',seqid)\n",
    "        print('representative dataset:\\t',ds)\n",
    "        \n",
    "    n += 1\n",
    "if verbose:\n",
    "    print(dataset_lookup)\n",
    "#print(sequence_collector)\n",
    "print(\"There are\",len(dataset_lookup), 'datasets and',total_seq_count,'total sequences')\n",
    "print('Done')\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running....\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Panel-06\n",
    "CREATE CLEAN FASTA FILES\n",
    "Output: seqfile.fa in each 'work/ds' directory\n",
    "TODO: Output unique seqs and counts directly rather than use 'mothur'\n",
    "\"\"\"\n",
    "print('Running....')\n",
    "# delete and re-create entire 'work' directory\n",
    "if os.path.exists(work_dir):\n",
    "    shutil.rmtree(work_dir) \n",
    "os.mkdir(work_dir)\n",
    "\n",
    "# create directory structure\n",
    "for ds in dataset_lookup:\n",
    "    if not os.path.exists(os.path.join(work_dir,ds)):\n",
    "        os.makedirs(os.path.join(work_dir,ds))\n",
    "        \n",
    "# write clean individual fasta files:    \n",
    "for ds in sequence_collector:\n",
    "    cleanfile = os.path.join(work_dir,ds,'seqfile.fa')\n",
    "    f = open(cleanfile, 'w')\n",
    "    for seqid in sequence_collector[ds]:\n",
    "        seq = sequence_collector[ds][seqid][:]        \n",
    "        f.write('>'+seqid+'\\n'+seq+'\\n')\n",
    "    f.close()\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running....\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "\"\"\" \n",
    "Panel-07\n",
    "UNIQUE Each Sequence file(s)\n",
    "# using mothur\n",
    "# mothur_cmd = mothur \\\"#unique.seqs(fasta=$FASTA)\\\"\n",
    "Output: uniques.fa and names file in each 'work/ds' directory\n",
    "\"\"\"\n",
    "print('Running....')\n",
    "for ds in dataset_lookup:\n",
    "    os.chdir(os.path.join(work_dir,ds))\n",
    "    seqfile = 'seqfile.fa'\n",
    "    out_pts = seqfile.split('.')\n",
    "    uniques_file = '.'.join(out_pts[:-1])+'.unique.'+out_pts[-1]\n",
    "    names_file = '.'.join(out_pts[:-1])+'.names'\n",
    "    mothur_cmd = \"mothur \\\"#unique.seqs(fasta=\"+seqfile+\")\\\"\"\n",
    "    os.system(mothur_cmd)\n",
    "    os.rename(uniques_file,'uniques.fa')\n",
    "    os.rename(names_file,'names')\n",
    "    os.chdir(base_dir)\n",
    "os.chdir(base_dir)\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running....\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Panel-08\n",
    "ASSIGN Taxonomy\n",
    "Output: rdpfile file in each 'work/ds' directory\n",
    "\"\"\"\n",
    "print('Running....')\n",
    "if taxonomy_assigner == 'rdp':\n",
    "    # RDP: Choose gene from: 16srrna, fungallsu, fungalits_warcup, fungalits_unite\n",
    "    rdp_gene = '16srrna' # for Euks choose either of: fungallsu, fungalits_warcup, fungalits_unite\n",
    "    # See \"java -Xmx4000M -jar ../rdp_classifier/dist/classifier.jar -h\" for RDP help\n",
    "    rdp_jar_path = os.path.join(base_dir,'..','rdp_classifier','dist','classifier.jar')\n",
    "\n",
    "    for ds in dataset_lookup:\n",
    "        uniques_file = os.path.join(work_dir,ds,'uniques.fa')\n",
    "        rdp_outfile = os.path.join(work_dir,ds,'rdpfile')\n",
    "        rdp_cmd = 'java -Xmx4000M -jar '+rdp_jar_path+\" -g \"+rdp_gene+\" -o \"+rdp_outfile+\" -f fixrank \"+ uniques_file\n",
    "        #print(rdp_cmd)\n",
    "        os.system(rdp_cmd)\n",
    "    print('Done')\n",
    "else:\n",
    "    print('No useful tax assigner')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running....\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Panel-09\n",
    "PARSE RDP FILE(S) and WRITE OUTPUT\n",
    "Output: vamps_data.csv file in each 'work/ds' directory\n",
    "\"\"\"\n",
    "print('Running....')\n",
    "\n",
    "for ds in dataset_lookup:\n",
    "    rdp_file     = os.path.join(work_dir,ds,'rdpfile')\n",
    "    uniques_file = os.path.join(work_dir,ds,'uniques.fa')\n",
    "    names_file   = os.path.join(work_dir,ds,'names')\n",
    "    vampsfile = os.path.join(work_dir,ds,'vamps_data.csv')\n",
    "    obj = RDP_File_Parser(rdpFile=rdp_file, \n",
    "                    uniquesFile=uniques_file, \n",
    "                    namesFile=names_file, \n",
    "                    outFile=vampsfile,\n",
    "                    min_boot=0.8)\n",
    "    obj.parse_file()\n",
    "    obj.write_file()\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RANK_COLLECTOR:\n",
      "{'genus': 6, 'family': 5, 'domain': 1, 'strain': 8, 'order': 4, 'klass': 3, 'phylum': 2, 'species': 7}\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Panel-10\n",
    "DATABASE Step-1: create RANK_COLLECTOR\n",
    "\"\"\"\n",
    "RANK_COLLECTOR         = {}\n",
    "SEQ_COLLECTOR          = {}\n",
    "#SUMMED_TAX_COLLECTOR   = {}\n",
    "#TAX_ID_BY_RANKID_N_TAX = {}\n",
    "#SILVA_IDS_BY_TAX       = {}\n",
    "DATASET_ID_BY_NAME     = {}\n",
    "ranks    = ['domain','phylum','klass','order','family','genus','species','strain']\n",
    "rank_ids = ['domain_id','phylum_id','klass_id','order_id','family_id','genus_id','species_id','strain_id']\n",
    "accepted_domains = ['bacteria','archaea','eukarya','fungi','organelle','unknown']\n",
    "sqlranks = \"','\".join(ranks)       \n",
    "q = \"SELECT rank,rank_id FROM rank WHERE rank in ('%s')\" % (sqlranks)\n",
    "if verbose: print(q)\n",
    "db.cur.execute(q)\n",
    "rows = db.cur.fetchall()\n",
    "for row in rows:\n",
    "    RANK_COLLECTOR[row[0]] = row[1]\n",
    "print('RANK_COLLECTOR:')\n",
    "print(RANK_COLLECTOR)\n",
    "print('Done')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running....\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Panel-11\n",
    "DATABASE Step-2\n",
    "Read VAMPS File(s) and Push Taxonomy\n",
    "\"\"\"\n",
    "print('Running....')\n",
    "#import csv\n",
    "#lines = csv.reader(open(file_info['name'],\"r\"), delimiter='\\t')\n",
    "for ds in dataset_lookup:\n",
    "    vampsfile = os.path.join(work_dir,ds,'vamps_data.csv')\n",
    "    tax_collector = {}\n",
    "    if ds not in SEQ_COLLECTOR:\n",
    "        SEQ_COLLECTOR[ds] = {}\n",
    "    #if ds not in SUMMED_TAX_COLLECTOR:\n",
    "    #    SUMMED_TAX_COLLECTOR[ds]={}\n",
    "    with open(vampsfile) as f:\n",
    "        for line in f:\n",
    "            items = line.strip().split('\\t')\n",
    "            seqid       = items[0]\n",
    "            seq         = items[1]\n",
    "            tax_string  = items[2]\n",
    "            boot_score  = items[3]\n",
    "            rank        = items[4]\n",
    "            seq_count   = items[5]\n",
    "            if rank == 'class': rank = 'klass' \n",
    "            if rank == 'orderx': rank = 'order'\n",
    "            SEQ_COLLECTOR[ds][seq] = {'dataset':ds,                                  \n",
    "                                      'taxonomy':tax_string,    \n",
    "                                      'rank':rank,     \n",
    "                                      'seq_count':seq_count,   \n",
    "                                      'boot':boot_score\n",
    "                                     }    \n",
    "            SEQ_COLLECTOR[ds][seq]['rank_id'] = RANK_COLLECTOR[rank]\n",
    "            tax_items = tax_string.split(';')\n",
    "#             sumtax = ''\n",
    "#             for i in range(0,8):\n",
    "#                 rank_id = RANK_COLLECTOR[ranks[i]]\n",
    "#                 if len(tax_items) > i:\n",
    "#                     taxitem = tax_items[i]\n",
    "#                 else:\n",
    "#                     taxitem = ranks[i]+'_NA'\n",
    "#                 sumtax += taxitem+';'\n",
    "\n",
    "#                 #print ranks[i],rank_id,taxitem,sumtax,seq_count\n",
    "#                 if rank_id in SUMMED_TAX_COLLECTOR[ds]:\n",
    "#                     if sumtax[:-1] in SUMMED_TAX_COLLECTOR[ds][rank_id]:\n",
    "#                         SUMMED_TAX_COLLECTOR[ds][rank_id][sumtax[:-1]] += int(seq_count)\n",
    "#                     else:\n",
    "#                         SUMMED_TAX_COLLECTOR[ds][rank_id][sumtax[:-1]] = int(seq_count)\n",
    "#                 else:\n",
    "#                     SUMMED_TAX_COLLECTOR[ds][rank_id] = {}\n",
    "#                     SUMMED_TAX_COLLECTOR[ds][rank_id][sumtax[:-1]] = int(seq_count)\n",
    "                \n",
    "                \n",
    "            if tax_items[0].lower() in accepted_domains:\n",
    "                ids_by_rank = []\n",
    "                for i in range(0,8):\n",
    "                    #print i,len(tax_items),tax_items[i]\n",
    "                    rank_name = ranks[i]\n",
    "                    rank_id = RANK_COLLECTOR[ranks[i]]\n",
    "\n",
    "                    if len(tax_items) > i:\n",
    "                        if ranks[i] == 'species':\n",
    "                            t = tax_items[i].lower()\n",
    "                        else:\n",
    "                            t = tax_items[i].capitalize()\n",
    "\n",
    "                        if tax_items[i].lower() != (rank_name+'_NA').lower():\n",
    "                            name_found = False\n",
    "                            if rank_name in tax_collector:\n",
    "                                tax_collector[rank_name].append(t)\n",
    "                            else:\n",
    "                                tax_collector[rank_name] = [t]\n",
    "                    else:\n",
    "                        t = rank_name+'_NA'\n",
    "\n",
    "\n",
    "\n",
    "                    q2 = \"INSERT IGNORE INTO `\"+rank_name+\"` (`\"+rank_name+\"`) VALUES('\"+t+\"')\"\n",
    "                    if verbose: print(q2)\n",
    "                    db.cur.execute(q2)\n",
    "                    db.conn.commit()\n",
    "                    tax_id = db.cur.lastrowid\n",
    "                    if tax_id == 0:\n",
    "                        q3 = \"SELECT \"+rank_name+\"_id FROM `\"+rank_name+\"` WHERE `\"+rank_name+\"` = '\"+t+\"'\"\n",
    "                        if verbose: print( q3 )\n",
    "                        db.cur.execute(q3)\n",
    "                        db.conn.commit()\n",
    "                        row = db.cur.fetchone()\n",
    "                        tax_id=row[0]\n",
    "                    ids_by_rank.append(str(tax_id))\n",
    "                    \n",
    "                    if verbose: print( 'rank_id,t,tax_id '+str(rank_id)+' - '+t+' - '+str(tax_id)  )\n",
    "#                     if rank_id in TAX_ID_BY_RANKID_N_TAX:\n",
    "#                         TAX_ID_BY_RANKID_N_TAX[rank_id][t] = tax_id\n",
    "#                     else:\n",
    "#                         TAX_ID_BY_RANKID_N_TAX[rank_id]={}\n",
    "#                         TAX_ID_BY_RANKID_N_TAX[rank_id][t] = tax_id\n",
    "                    #ids_by_rank.append('1')\n",
    "                if verbose: print(ids_by_rank )\n",
    "                q4 =  \"INSERT IGNORE INTO silva_taxonomy (\"+','.join(rank_ids)+\",created_at)\"\n",
    "                q4 += \" VALUES(\"+','.join(ids_by_rank)+\",CURRENT_TIMESTAMP())\"\n",
    "                if verbose: print(q4)\n",
    "                db.cur.execute(q4)\n",
    "                db.conn.commit()\n",
    "                silva_tax_id = db.cur.lastrowid\n",
    "                if silva_tax_id == 0:\n",
    "                    q5 = \"SELECT silva_taxonomy_id FROM silva_taxonomy WHERE (\"\n",
    "                    vals = ''\n",
    "                    for i in range(0,len(rank_ids)):\n",
    "                        vals += ' '+rank_ids[i]+\"=\"+ids_by_rank[i]+' AND'\n",
    "                    q5 = q5 + vals[0:-3] + ')'\n",
    "                    if verbose: print(q5)\n",
    "                    db.cur.execute(q5)\n",
    "                    db.conn.commit()\n",
    "                    row = db.cur.fetchone()\n",
    "                    silva_tax_id=row[0]\n",
    "\n",
    "                #SILVA_IDS_BY_TAX[tax_string] = silva_tax_id\n",
    "                SEQ_COLLECTOR[ds][seq]['silva_tax_id'] = silva_tax_id\n",
    "                db.conn.commit()\n",
    "\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running....\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Panel-12\n",
    "DATABASE Step-3\n",
    "Push Sequences\n",
    "\"\"\"\n",
    "print('Running....')\n",
    "for ds in dataset_lookup:\n",
    "    for seq in SEQ_COLLECTOR[ds]:\n",
    "        q = \"INSERT IGNORE INTO sequence (sequence_comp) VALUES (COMPRESS('\"+seq+\"'))\"\n",
    "        if verbose: print(q)\n",
    "        db.cur.execute(q)\n",
    "        db.conn.commit()\n",
    "        seqid = db.cur.lastrowid\n",
    "        if seqid == 0:\n",
    "            q2 = \"SELECT sequence_id FROM sequence WHERE sequence_comp = COMPRESS('\"+seq+\"')\"\n",
    "            if verbose: print('DUP SEQ FOUND')\n",
    "            db.cur.execute(q2)\n",
    "            db.conn.commit()\n",
    "            row = db.cur.fetchone()\n",
    "            seqid=row[0]\n",
    "        SEQ_COLLECTOR[ds][seq]['sequence_id'] = seqid\n",
    "        silva_tax_id = str(SEQ_COLLECTOR[ds][seq]['silva_tax_id'])\n",
    "        distance = str(SEQ_COLLECTOR[ds][seq]['boot'])\n",
    "        if verbose: print( ds+' - '+seq+' - '+str(silva_tax_id))\n",
    "        rank_id = str(SEQ_COLLECTOR[ds][seq]['rank_id'])\n",
    "        if verbose: print( rank_id)\n",
    "        q = \"INSERT IGNORE INTO silva_taxonomy_info_per_seq\"\n",
    "        q += \" (sequence_id,silva_taxonomy_id,gast_distance,refssu_id,rank_id)\"\n",
    "        q += \" VALUES ('\"+str(seqid)+\"','\"+silva_tax_id+\"','\"+distance+\"','0','\"+rank_id+\"')\"\n",
    "        if verbose: print(q)\n",
    "        db.cur.execute(q)\n",
    "        db.conn.commit()\n",
    "        silva_tax_seq_id = db.cur.lastrowid\n",
    "        if verbose: print('1: '+str(silva_tax_seq_id))\n",
    "        if silva_tax_seq_id == 0:\n",
    "            q3 = \"SELECT silva_taxonomy_info_per_seq_id FROM silva_taxonomy_info_per_seq\"\n",
    "            q3 += \" WHERE sequence_id = '\"+str(seqid)+\"'\"\n",
    "            if verbose: print('DUP silva_tax_seq')\n",
    "            if verbose: print(q3)\n",
    "            db.cur.execute(q3)\n",
    "            db.conn.commit()\n",
    "            row = db.cur.fetchone()\n",
    "            if verbose: print(row)\n",
    "            silva_tax_seq_id=row[0]\n",
    "            if verbose: print('0: '+str(silva_tax_seq_id))\n",
    "\n",
    "        q4 = \"INSERT IGNORE INTO sequence_uniq_info (sequence_id, silva_taxonomy_info_per_seq_id)\"\n",
    "        q4 += \" VALUES('\"+str(seqid)+\"','\"+str(silva_tax_seq_id)+\"')\"\n",
    "        if verbose: print(q4)\n",
    "        db.cur.execute(q4)\n",
    "        db.conn.commit()\n",
    "        ## don't see that we need to save uniq_ids\n",
    "    db.conn.commit()\n",
    "    print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INSERT INTO project (project,title,project_description,rev_project_name,funding,owner_user_id,public) VALUES('t18_637','testing title','testing desc','736_81t','01234','48','1')\n",
      "NEW Project: t18_637 (PID = 112)\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Panel-13\n",
    "DATABASE Step-4\n",
    "Push Project\n",
    "\"\"\"\n",
    "desc  = prj_info[\"description\"]\n",
    "title = prj_info[\"title\"]\n",
    "proj  = prj_info['name']\n",
    "rev   = prj_info['name'][::-1]\n",
    "fund  = prj_info[\"funding\"]\n",
    "oid   = prj_info['owner_id']\n",
    "pub   = prj_info['public']\n",
    "fields = ['project','title','project_description','rev_project_name','funding','owner_user_id','public']\n",
    "\n",
    "q = \"INSERT INTO project (\"+(',').join(fields)+\")\"\n",
    "q += \" VALUES('%s','%s','%s','%s','%s','%s','%s')\"\n",
    "q = q % (proj, title, desc, rev, fund, oid, pub)\n",
    "print(q)\n",
    "db.cur.execute(q)\n",
    "db.conn.commit()\n",
    "prj_info['project_id'] = db.cur.lastrowid\n",
    "print(\"NEW Project: \"+prj_info['name']+\" (PID = \"+str(prj_info['project_id'])+')')\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATASET_ID_BY_NAME:\n",
      "{'ds1': '859'}\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Panel-14\n",
    "DATABASE Step-5\n",
    "Push Dataset(s)\n",
    "\"\"\"\n",
    "fields = ['dataset','dataset_description','project_id']\n",
    "for ds in dataset_lookup:\n",
    "    q = \"INSERT INTO dataset (\"+(',').join(fields)+\")\"\n",
    "    q += \" VALUES('%s','%s','%s')\"\n",
    "    #print ds,desc,CONFIG_ITEMS['env_source_id'],CONFIG_ITEMS['project_id']\n",
    "    q4 = q % (ds, ds+'_description', prj_info['project_id'])\n",
    "    if verbose: print(q4)\n",
    "    db.cur.execute(q4)\n",
    "    did = db.cur.lastrowid\n",
    "    DATASET_ID_BY_NAME[ds] = str(did)\n",
    "    db.conn.commit()\n",
    "    print(\"DATASET_ID_BY_NAME:\")\n",
    "    print(DATASET_ID_BY_NAME)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Panel-15\n",
    "DATABASE Step-6\n",
    "Push PDR Seqs\n",
    "\"\"\"\n",
    "classifier_id = '5' # '5' for rdp; '2' for gast (on local) See classifier table\n",
    "for ds in dataset_lookup: \n",
    "    did = DATASET_ID_BY_NAME[ds]\n",
    "    for seq in SEQ_COLLECTOR[ds]:        \n",
    "        seqid = SEQ_COLLECTOR[ds][seq]['sequence_id']\n",
    "        count = SEQ_COLLECTOR[ds][seq]['seq_count']\n",
    "        q = \"INSERT INTO sequence_pdr_info (dataset_id, sequence_id, seq_count, classifier_id)\"\n",
    "        q += \" VALUES ('\"+str(did)+\"','\"+str(seqid)+\"','\"+str(count)+\"','\"+classifier_id+\"')\"\n",
    "        if verbose: print(q)\n",
    "        db.cur.execute(q)\n",
    "    db.conn.commit()\n",
    "print(\"Done\")\n",
    "# from bin.load_to_database import Load_CSV_Data_File\n",
    "# for ds in dataset_lookup:\n",
    "    \n",
    "#     vampsfile = os.path.join(work_dir,ds,'vamps_data.csv')\n",
    "#     obj = Load_CSV_Data_File(inFile=vampsfile,\n",
    "#                              project=prj_info[\"name\"],\n",
    "#                              dataset=ds,\n",
    "#                              user=prj_info[\"owner\"],\n",
    "#                              public=prj_info[\"public\"],\n",
    "#                              dbhost=db_info['host'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no vals for  {'table': 'term', 'value': 'NWCS', 'name': 'geo_loc_name'}\n",
      "{'adapter_sequence_id': 1, 'latitude': '41.525480', 'illumina_index_id': 83, 'env_matter_id': 1280, 'dna_region_id': 12, 'geo_loc_name_id': 9999, 'longitude': '-70.674854', 'sequencing_platform_id': 2, 'domain_id': 140108, 'env_biome_id': 1375, 'env_feature_id': 9999, 'run_id': 5543, 'env_package_id': 22, 'collection_date': '2016-12-25', 'target_gene_id': 1, 'primer_suite_id': 23}\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Panel-16 \n",
    "Metadata (Required)\n",
    "TODO: Fix This::Assume here that all the datasets get the same required metadata\n",
    "\"\"\"\n",
    "collector = {}\n",
    "# items without ids:\n",
    "collector['collection_date'] = \"2016-12-25\"\n",
    "collector['latitude']        = \"41.525480\"\n",
    "collector['longitude']       = \"-70.674854\"\n",
    "\n",
    "# items with ids:\n",
    "req_md2 = [\n",
    "     {\"table\":\"term\",               \"name\":\"env_biome\",          \"value\":\"sea water\"},         # term     \n",
    "     {\"table\":\"target_gene\",        \"name\":\"target_gene\",        \"value\":\"16s\"},               # target_gene\n",
    "     {\"table\":\"dna_region\",         \"name\":\"dna_region\" ,        \"value\":\"v6\"},                # dna_region\n",
    "     {\"table\":\"sequencing_platform\",\"name\":\"sequencing_platform\",\"value\":\"illumina\"},          # sequencing_platform\n",
    "     {\"table\":\"domain\",             \"name\":\"domain\",             \"value\":\"bacteria\"},          # domain\n",
    "     {\"table\":\"term\",               \"name\":\"geo_loc_name\",       \"value\":\"NWCS\"},              # term\n",
    "     {\"table\":\"term\",               \"name\":\"env_feature\",        \"value\":\"unknown\"},           # term\n",
    "     {\"table\":\"term\",               \"name\":\"env_matter\",         \"value\":\"water\"},             # term\n",
    "     {\"table\":\"env_package\",        \"name\":\"env_package\",        \"value\":\"water-marine\"},      # env_package\n",
    "     {\"table\":\"run_key\",            \"name\":\"adapter_sequence\",   \"value\":\"unknown\"},           # run_key\n",
    "     {\"table\":\"illumina_index\",     \"name\":\"illumina_index\",     \"value\":\"unknown\"},           # illumina_index\n",
    "     {\"table\":\"primer_suite\",       \"name\":\"primer_suite\",       \"value\":\"Bacterial v6 Suite\"},# primer_suite\n",
    "     {\"table\":\"run\",                \"name\":\"run\",                \"value\":\"unknown\"}            # run\n",
    "]\n",
    "\n",
    "# get the ids from the names/values\n",
    "for i,q in enumerate(req_md2):\n",
    "    if req_md2[i][\"table\"] == 'term':\n",
    "        name = 'term_name'\n",
    "        id = 'term_id'\n",
    "    elif req_md2[i][\"table\"] == 'run_key':\n",
    "        name = 'run_key'\n",
    "        id = 'run_key_id'        \n",
    "    else:\n",
    "        name = req_md2[i][\"name\"]\n",
    "        id = name+\"_id\"\n",
    "    q = \"SELECT \"+id+\", \"+name\n",
    "    q += \" FROM \"+req_md2[i][\"table\"] \n",
    "    q += \" WHERE \"+name+\" = '\"+req_md2[i][\"value\"]+\"';\"\n",
    "    if verbose: print(q)\n",
    "    db.cur.execute(q)\n",
    "    rowcount = db.cur.rowcount\n",
    "    if rowcount == 0:\n",
    "        print('no vals for ',req_md2[i])\n",
    "        q2 = q[:q.find('=')]+\"='unknown'\" # ALL the tables should have 'unknown'\n",
    "        if verbose: print(q2)\n",
    "        db.cur.execute(q2)\n",
    "        row2 = db.cur.fetchone()\n",
    "        id = row2[0]\n",
    "    else:\n",
    "        row1 = db.cur.fetchone()\n",
    "        id = row1[0]\n",
    "         \n",
    "    collector[req_md2[i][\"name\"]+\"_id\"] = id\n",
    "    \n",
    "print(collector)\n",
    "set_order = collector.keys()\n",
    "set_values = []\n",
    "for name in set_order:\n",
    "    set_values.append(str(collector[name]))\n",
    "\n",
    "for ds in dataset_lookup:\n",
    "    did = DATASET_ID_BY_NAME[ds]\n",
    "    q = \"INSERT INTO required_metadata_info (dataset_id,\"+','.join(set_order)+\")\"\n",
    "    q += \" VALUES('%s',\"\n",
    "    for n in range(len(set_order)):\n",
    "        q += \"'%s',\"\n",
    "    q = q[:-1]\n",
    "    q += \")\"\n",
    "    q = q % tuple([did]+set_values)\n",
    "    if verbose: print(q)\n",
    "    try:\n",
    "        db.cur.execute(q)\n",
    "    except:\n",
    "        print(\"INSERT Failed:\\n\",q)\n",
    "db.conn.commit()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
